# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =
saveParticles = true

[plugins]
heuristicPlugin = libaudioClassificationHeuristicPlugin.so

planningRewardPlugin = libaudioClassificationRewardPlugin.so
executionRewardPlugin = libaudioClassificationRewardPlugin.so

planningTerminalPlugin = libaudioClassificationTerminalPlugin.so
executionTerminalPlugin = libaudioClassificationTerminalPlugin.so

planningTransitionPlugin = libaudioClassificationTransitionPlugin.so
executionTransitionPlugin = libaudioClassificationTransitionPlugin.so

planningObservationPlugin = libaudioClassificationObservationPlugin.so
executionObservationPlugin = libaudioClassificationObservationPluginExecution.so

executionInitialBeliefPlugin = libaudioClassificationInitialBeliefPlugin.so
planningInitialBeliefPlugin = libaudioClassificationInitialBeliefPlugin.so

[movoOptions]
urdfFile = movo.urdf
baseLink = base_link
endEffectorLink = right_ee_link
cupLink = cup::cup_base_link
localIP = 10.66.171.190

[initialBeliefOptions]
# First 7 dimensions: initial joint angles
# Next 3 dimensions: object position relative to the end-effector pose
# Last dimension: object property
initialState = [-2.89527, 1.45058, -0.18396, 2.07696, -0.310356, 0.630897, -1.54233, 0.1, 0.0, 0.0, 1.0]

# The initial distribution over the object property (e.g. object type)
# [0.5, 0.5] means that we have two object types, both occuring with equal probability (0.5).
# [0.7, 0.1, 0.2] would mean that we have three object types where the first one occurs with a probability
# of 0.7, etc.
initialObjectPropertyBelief = [0.25, 0.25, 0.25, 0.25]

[transitionPluginOptions]
# The distance (in meters) the end effector travels for the X_PLUS, X_MINUS, Y_PLUS and Y_MINUS actions
endEffectorMotionDistance = 0.05

[observationPluginOptions]
correctObservationProbability = 0.99

[rewardPluginOptions]
#stepPenalty = 1.0
#illegalMovePenalty = 1000.0
#exitReward = 1000.0
#scanningPenalty = 10.0

stepPenalty = 1.0
illegalMovePenalty = 1000.0
exitReward = 1000.0
scanningPenalty = 10.0

[heuristicPluginOptions]
scalingFactor = 1.0

[problem]
robotName = movo

# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The planning environment SDF
planningEnvironmentPath = MovoEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = MovoEnvironment.sdf

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.99

allowCollisions = true

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 2000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
# First seven dimensions of the state are the joint angles
jointPositions = [movo::right_shoulder_pan_joint, movo::right_shoulder_lift_joint, movo::right_arm_half_joint, movo::right_elbow_joint, movo::right_wrist_spherical_1_joint, movo::right_wrist_spherical_2_joint, movo::right_wrist_3_joint]

# Next three dimensions is the end effector position relative to the end-effector. The last dimesion
# describes the actual object (cup or mug)
additionalDimemsions = 4

[action]
additionalDimensions = 1

# Encodes 13 discrete actions
additionalDimensionLimits = [[0.0, 1.0]]

[observation]
additionalDimensions = 1
#additionalDimensionLimits = [[0.0, 1.0]]

[changes]
hasChanges = false
changesPath = changes_1.txt
areDynamic = false

[options]
collisionInvariantLinks = [movo::base_link, movo::linear_actuator_link, movo::tilt_link, movo::pan_link, movo::left_shoulder_link, movo::left_arm_half_1_link, movo::left_arm_half_2_link, movo::left_forearm_link, movo::left_wrist_spherical_1_link, movo::left_wrist_spherical_2_link, movo::left_wrist_3_link, movo::left_gripper_finger1_knuckle_link, movo::left_gripper_finger2_knuckle_link, movo::left_gripper_finger3_knuckle_link]

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 1000

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 0.9

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

maximumDepth = 5000

searchStrategy = ucb(90.0)

# For ContAbt
ucbExplorationFactor = 200.0

estimator = mean()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 13

observationType = continuous
numInputStepsObservation = 3

[simulation]
interactive = false
particlePlotLimit = 1

